> 文章摘要：**随着电商业务越做越大，原来开放平台的 10 库的容量日益满足不了业务快速发展的需要，为了保障今年 11 月份大促后库业务正常运行，需要提前把订单库的 10 库扩容为 50 库。本文我们就来讲述下扩容流程。**

## 一、前言

随着电商业务越做越大，原来开放平台的 10 库的容量日益满足不了业务快速发展的需要，为了保障今年 11 月份大促后库业务正常运行，需要提前把订单库的 10 库扩容为 50 库。本文我们就来讲述下扩容流程。

## 二、开放平台订单库原来分库分表规则
如果订单库是按照标准分库分表规则来做的，那么本文就没必要存在了，因为如果是标准的，DBA 这边有工具可以很便捷的进行扩容。

我们订单库总共有 9 张表，每个表有 1000 个分表，总计 9000 张表。

首先我们来看下我们 10 库的订单库分库分表规则：

|库编号|	分表编号|描述|
|---|---|---|
|库 0|	0-99|	
|库 1	|100-199|
|库 2|	200-299	|
|库 3|300-399|
|库 4|400-499|
|库 5|	500-599	|
|库 6|	600-699	|
|库 7|	700-799	|
|库 8|	800-899	|
|库 9|	900-999	|
如上可知，我们原来分库分表是按照分表的编号来分段，即：

- 第一个库放表编号为 0-99 的 100 张表；
- 第二个库放表编号为 100-199 的 100 张表；
- ...
- 第 10 个库方表编号为 900-999 的 100 张表；


## 三、DBA 推荐的标准分库分表规则与扩容流程
DBA 这边有 标准的分库分表规则， 下面我们只看 10 库分表下分库与分表的映射关系为：

|库编号|	表后缀	|描述|
|---|---|---|
|库 0|	\d*[0]$	|
|库 1|	\d*[1]$	|
|库 2|	\d*[2]$	|
|库 3|	\d*[3]$	|
|库 4|	\d*[4]$	|
|库 5|	\d*[5]$	|
|库 6|	\d*[6]$	|
|库 7|	\d*[7]$	|
|库 8|	\d*[8]$	|
|库 9|	\d*[9]$	|

如上可知，标准分库分表是按照表的后缀进行正则匹配来映射某个库应该含有哪些表，即：

- 第一个库放表后缀为 0 的表，比如 table_0、table_10、table_20,...，table_990；
- 第二个库放表后缀为 1 的表，比如 table_1、table_11、table_21,...，table_991；
- ...
- 第 10 个库放表后缀为 9 的表，比如 table_9、table_19、table_29,...，table_999；


对于这种标准的 10 库分表来说，扩到 50 库很容易，因为 DBA 这边可以把每个库的 100 表使用工具拆分为 5 个库（每个库 20 个表），具体迁移步骤很简单，如：多库扩容多库 。这种情况下，我们业务只需要配合 DBA 校验 zk 配置即可，具体老库到新库的数据迁移 DBA 这边用工具自动来做。

这里我们概要看下如何把库 0 里面的 table_0、table_10、table_20,...，table_990 表拆为五个库：

|库名|	后缀|	描述|
|---|---|---|
|库 0	|00 或 0、10|	
|库 10	|20、30	|
|库 20	|40、50	|
|库 30	|60、70	|
|库 40	|80、90	|

## 四、开放平台订单库扩容为何不能自动扩？

- 第一，无规矩不成方圆，分库分表规则的标准化，便于自动化管理，比如自动数据迁移之类的。试想如果每个业务自己的分库标准各异，则 DBA 这边很难有一套工具来进行覆盖管理。所以 DBA 这边指定了标准分库规则。所以我们订单库扩容后的 50 库，也要符合标准的分库规则。


- 第二，因为我们现在 10 库的分库规则不是标准的，但是要扩容为标准规则的 50 库。这需要 DBA 把原来每个库里面的 100 个表分别映射到 50 个不同的库。而我们订单库里面有 9 个表，每个表都是千分表。而 DBA 这边目前并没有这种工具来做这个事情。与基础架构和 DBA 这边聊后，最终结论是：**我们来自己做老库到新库的数据迁移，以及新老库数据源切换操作。**


## 五、非标准分库 10 库到 50 库扩容流程
我们订单库扩容俨然变成了第一步进行数据迁移，然后进行数据源切换了，整个扩容流程变为了：

1. 业务申请新库（50 库），建好表，zk 上配好标准配置
2. 业务自己写工具，做老库表数据到新库表的迁移-进行全量数据迁移
3. 业务修改代码，进行双写（添加开关控制），读的地方也进行开关控制（但是先读老库）-进行增量数据迁移。
4. 步骤 3 完毕后，进行新老库数据对比
5. 步骤 4 对比完成后，打开读开关读取新库，读取一段时间，没问题，则切写新库。


**整体思路是：**

1. 先做老库里面的某个表到新库对应表的全量迁移，具体如何做下面会讲解

2. 全量迁移的同时，打开增量同步，增量过程一直开启，直到写也切到了新库。至于如何做增量，下面会具体讲解

3. 等全量数据迁移完毕后，执行兜底任务进行数据补偿，为啥要补偿那，因为在全量过程中我们丢失了一部分数据，所以我们要把全量开始和全量结束期间修改的数据进行补偿，补偿方案下面讲解。

4. 等补偿任务执行完毕后，我们就可以对新老库中数据进行对比，对比方案下面讲解。

5. 对比结束后，进行动态数据源切换

### 5.1 全量迁移任务
第一步我们要把老库里面的 9000 张表，总计 7T 的全量数据迁移到新库，由于数据量很大，我们采用逐步迭代方式进行迁移，也就是每次迁移 1000 张表（单表的 1000 个分表），迁移完毕后，在进行该千表的兜底任务、数据对比工作，数据对比 OK，则进行下个 1000 表的处理。


对于某个表来说，全量迁移就是把当前该表里面的所有数据全部从老库迁移到新库对应表里面，由于一个表里面数据很多，不可能一次全部加载到内存里面，因为这样为 OOM。


我们采用循环迭代的方式，每次从 DB 迭代出 limit 条记录，具体是检索数据时按照主键 id 逆序排序，然后搜出主键 id 小于 cursor(初始化为 Long 最大值) 的 limit 条记录，然后在拿出 limit 条记录中最小的主键 id 计算当前 cursor，再次进行迭代...，该策略的具体实现是使用了基础架构提供的：

```java
Stream<T> listAllDesc(long shardId) 
```

该方法每次从源表默认迭代 200 条数据，由于默认是每次迭代处 200 条，所以可以自己稍加改造，添加 limitSize 参数来控制每次批量迭代多少条：

```java
Stream<T> listAllDesc(long shardId, int limitSize) 
```

该方法返回了一个 JDK8Stream，代码拿到源数据源表的流对象后，我们就需要从流中读取记录然后插入到目的表了，鉴于性能和迁移速度考虑，我们使用了批量插入，也就是每次从流中聚合一定数量的记录后再执行批量插入，具体如何从流便捷的聚合消费可以参考：[如何从 JDK8 Stream 转换为反应式框架流？](https://kstack.corp.kuaishou.com/tech/web/article/info/1186)


另外每个全量迁移任务，迁移的是某个表的千表，我们可以通过并发数来控制同时有多少分表同时进行迁移，另外使用 Guava 的 RateLimiter 进行背压限速 （控制新库的写入速度，从而控制迭代老库的迭代速速）。


**全量迁移过程中注意事项：**

- mysql 服务端默认 max_allowed_packet=4M，我们每次从源库读取 1000 条记录，有些表里面有 text 字段，会导致迭代处的数据量大于 4M，会导致抛出 Caused by: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (6705037 > 4194304). You can change this value on the server by setting the max_allowed_packet' variable.异常。可联系 DBA 把该参数改大。

- 全量批量插入时使用 INSERT IGNORE INTO 而不是 INSERT INTO ，这是因为在全量迁移的同时，增量也在进行，有可能增量时插入了一个新记录，会导致批量插入失败。因为批量插入的数据是根据主键 id 排序的，而可能存在主键 id 比较大，但是 updatetime 比较老的记录（updatetime 时间非常小于开启全量迁移时的时间），如果存在这样的记录，则兜底补偿任务会补偿不到，导致数据丢失。而使用 INSERT IGNORE INTO 则会忽略 DB 中已经存在的记录。

- 另外我们全量迁移的 9 个表的的逻辑是一致的，只是操作的 DAO 不同，所以简单的抽取下代码，减少代码重复量：

```java
private <T> void doSync(long shardId, Range range, OpenPlatformOrderBaseDao oldDao, OpenPlatformNewOrderBaseDao newDao, SyncTaskContext syncTaskContext) {
    //1.获取源数据库表的流对象
    int limit = syncTaskContext.getFetchSize() > 0 ? syncTaskContext.getFetchSize() : (Integer)OpenPlatformIntegerConfigKey.openPlatformOrderInfoSyncFetchLimit.get();
    Stream<T> stream = null;
    if (null == range) {
        stream = oldDao.listAllDesc(shardId, limit);
    } else if (syncTaskContext.isCompensationByUpdateTime()) {
        stream = oldDao.listByUpdateTimeRangeDesc(shardId, range, limit);
    } 
    //2.转换为reactor流
    stream = stream.filter(Objects::nonNull);
    Flux<T> flux = Flux.fromStream(stream);
    //3.批量消费插入新数据源
    int batchNum = syncTaskContext.getBatchInsertSize() > 0 ? syncTaskContext.getBatchInsertSize() : (Integer)OpenPlatformIntegerConfigKey.openPlatformOrderInfoSyncBatchNum.get();
    flux.buffer(batchNum).subscribe((infos) -> {
        try {
            WRITE_ALL_RATE_LIMITER.acquire();
            int result = newDao.insertIgnoreBatch(shardId, infos, (NamedParameterJdbcTemplate)null);
            log.info("{},sync,{},{},{}", new Object[]{oldDao.getTableName(shardId), shardId, infos.size(), result});

        }  catch (Exception var7) {
            log.error("{},sync error:", oldDao.getTableName(shardId), var7);
            MerchantPerfUtil.adMerchantPerf("data.migration.batch.insert.error", oldDao.getTableName(shardId), (String)null, (String)null, (String)null);
        }

    });
    ... 
}
```

### 5.2 增量迁移任务

前面 5.1 进行了历史数据的全量迁移，下面我们看如何进行增量数据的同步，对于实现增量，有下面几个方案

- 修改业务代码进行双写，也就是原来代码中写老库的地方同时进行写新库。
- 监听老库 binlog，然后更新新库。

对于 1 来说，我们需要修改代码中所有原来操作老库的地方，这个比较繁琐，很容易遗漏。对于 2 来说实现了新库增量更新与业务系统的解耦，所以我们选择了第二个方案：

对于第二个方法，消费 binlog 时为了保证同一个记录顺序处理，我们使用了 binlog 的亲缘性线程池进行处理，关于亲缘性线程池可以移步：[亲缘性线程池](https://kstack.corp.kuaishou.com/tech/web/article/info/1187)

对于 BinlogResolver 的使用，我们都是通过 Map<String, Serializable> rowMap = row.getAffectedRow()获取行记录，其中 rowMap 是行记录中所有列的 keyvalue 存储。对于行记录内容我们需要一个个按照列从 rowMap 中获取，这个是很费事并且容易出错的操作。而我们增量有 9 个表，大概几百个字段，很耗时，所以写了反射类，自动做字段映射，该方法根据传递的 rowMap：

```java
//反射设置对象，目前只有int/Integer,long/Long,string类型如有新类型，自行添加
private static <T> T getInfo(Map<String, Serializable> rowMap, Class<T> classObj, Set<String> textFieldList) throws Exception {

    //1.创建对象，并设置变量访问权限
    T info = classObj.newInstance();
    Field[] fields = classObj.getDeclaredFields();
    Field.setAccessible(fields, true);

    //2.获取变量值，并设置
    for (Field field : fields) {
        String fieldName = CaseFormat.LOWER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, field.getName());
        if (field.getType() == String.class) {
            if (null != textFieldList && textFieldList.contains(field.getName()) && null != rowMap.get(fieldName)) {
                field.set(info, new String((byte[]) rowMap.get(fieldName), Charset.forName("UTF-8")));
            } else {
                field.set(info, MapUtils.getString(rowMap, fieldName));
            }
        } else if (field.getType() == Integer.TYPE || field.getType() == Integer.class) {
            field.set(info, MapUtils.getInteger(rowMap, fieldName));
        } else if (field.getType() == Long.TYPE || field.getType() == Long.class) {
            field.set(info, MapUtils.getLong(rowMap, fieldName));
        }
    }

    return info;
}
```

这里需要注意的是对应 DB 中类型为 Text 类型的数据，从 binlog 里面解析时是二进制，要自己 特殊处理，转换为 String 进行消费。

### 5.3 兜底补偿任务
由于全量数据迁移是从当时最大主键 id 开始向下一直迭代数据，而在全量和增量开启中间可能插入或者更新了新数据，所以在全量数据迁移完毕后，要对整个全量过程中新插入的或者增量开启前的更新的数据进行兜底补偿。


我们的实现是根据 update_time 筛选出全量数据迁移过程中修改的数据（起始时间要略先与开始全全量开始的时间），然后更新或者插入到新库。

需要注意，这个过程中需要注意并发问题：因为基于 binlog 的增量任务也在更新新库，我们的兜底任务也在更新新库，这可能会存在数据覆盖问题。


比如我们兜底任务从老库迭代出来了 id=1,version=1 的数据，在更新到新库前，老库中的 id=1 的数据的版本被修改为了 version=2，然后通过增量任务的 binlog 更新到了新库，这时新库中 id=1 的数据版本为 2；但是兜底任务会把其更新为 version=1，从而导致最新的数据被覆盖。

解决之道是基于 update_time 做乐观锁控制：

```java
update table set ... where update_time <#updata_time。
```

### 5.4 数据对比任务
当兜底任务执行完毕后，理论上新老库数据是一致的。我们需要使用对比任务对新老数据进行对比。


对比思路：基于游标方式从老库迭代出一批数据，然后基于其 id 列表，从新库中迭代处对应数据，然后对比行记录内容，当数据一旦不一致，则退出当前对比任务中当前，进行排查。另外要适当加上监控埋点，以便当数据对比不一致时，查看不一致的原因，

需要注意由于增量方式基于 binlog 可能会存在延迟，造成数据对比不一致，这个可以通过对比失败后，在重新从新库查询一次来做一定补偿。另外对比数据不需要速度太快，可以通过 RateLimiter 进行限速，避免影响线上业务。

### 5.5 迁移监控
建立完善的监控，可以很好的观察整体迁移进度，迁移过程中出现的问题，出现问题的是那些表，等等。


监控全部全量或者兜底任务是否都执行好了：

监控那些表全量或兜底任务执行好了：

监控全量或兜底任务中已经执行完多少分表了：

监控全量和兜底任务执行失败情况：

监控增量迁移异常：

监控数据对比情况：

![image.png](1)

![image.png](2)

## 六 动态切库与切库监控
数据对比完成，下面就要进行切库了，如何进行切那？在我们应用中，所有的 DAO 都是继承

BaseDao 的，其代码结构如下：

```java
public interface XXXBaseDao<T> extends DeepShardDAOBaseInsert<T>, DeepShardDAOBaseGet<T> {

    default NamedParameterJdbcTemplate getReader(long shardId) {
           return MerchantOpenPlatformDataSource.merchantOrderOpen.shardRead(shardId);
    }

    default NamedParameterJdbcTemplate getWriter(long shardId) {
            return MerchantOpenPlatformDataSource.merchantOrderOpen.shardWrite(shardId);
    }
...
}
```

然后所有的 DAO 的 Write 操作都会调用其 getWriter(long shardId) 获取写数据源，所有 Reader 操作都会调用其 getReader(long shardId)方法获取读数据源。所以我们在这做了开关控制：

```java
public interface XXXBaseDao<T> extends DeepShardDAOBaseInsert<T>, DeepShardDAOBaseGet<T> {

    default NamedParameterJdbcTemplate getReader(long shardId) {
        if (OpenPlatformBooleanConfigKey.openNewOrderDb4ReadSwitch.get()) {
            return MerchantOpenPlatformDataSource.merchantOrderOpenNew.shardRead(shardId);
        } else {
            return MerchantOpenPlatformDataSource.merchantOrderOpen.shardRead(shardId);
        }
    }

    default NamedParameterJdbcTemplate getWriter(long shardId) {
        if (OpenPlatformBooleanConfigKey.openNewOrderDb4WriteSwitch.get()) {
            return MerchantOpenPlatformDataSource.merchantOrderOpenNew.shardWrite(shardId);
        } else {
            return MerchantOpenPlatformDataSource.merchantOrderOpen.shardWrite(shardId);
        }
    }
...
}
```

即通过开关 openNewOrderDb4ReadSwitch 控制切读库，通过开关 openNewOrderDb4WriteSwitch 控制切写库。这看起来很方便，但是需要注意的是，需要把用到该数据源 XXXBaseDao 的所有服务重新发布，这个不能有遗漏。

当数据对比完成后，我们就打开 openNewOrderDb4ReadSwitch 开关，让所有应用中的所有读操作都读取新库。等读验证一段时间 OK 后，打开 openNewOrderDb4WriteSwitch 开关，让所有应用写新库。另外为保证切读后，老库不会有读流量，切写库后，老库不会有写流量，我们加了大盘监控：

切读策略：可知如果有其他地方没有切到新库读，则由于存在binlog同步，不会出问题，可以找到这些服务，然后切到新库读

![image.png](3)

读切完毕后

![image.png](4)

切写策略：可知如果有其他地方没有切到新库写，则由于存在binlog同步，不会出问题，可以找到这些服务，然后切到新库写

![image.png](5)file

写切完毕后

![image.png](6)

如上我们可以知道当前库都有哪些服务在进行读写。

七、总结
总结下订单库的扩容操作还是比较繁琐的，后面如果有类似场景需要扩容，欢迎一起讨论。最后感谢基础架构组@杨昆，数据库组@梁贺两位大神，还有我们组@苏渤峰，@舒晓晖，@张怡颖，@郑炜，@徐迪磊，@祁盛旺等在扩容过程中的指导与帮助。

八、详细方案

[订单库扩容方案](https://wiki.corp.kuaishou.com/pages/viewpage.action?pageId=504456437)